{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=r'./pre_train.txt'\n",
    "test_data_path=r'./pre_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = ['gender','age','tagid','time','province','city','model','make']\n",
    "trained_features=['pid','label'] + common_features\n",
    "tested_features=['pid'] + common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(train_data_path, sep=',', header=None, names=trained_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(test_data_path,sep=',',header=None, names=tested_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [x for x in data.columns if x not in ['label']]:\n",
    "    data[col] = data[col].fillna(-1)\n",
    "    data[col] = data[col].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tagid'] = data['tagid'].apply(lambda x: eval(x)) # str -> list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time'] = data['time'].apply(lambda x: eval(x)) # str -> list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_rm_model(x):\n",
    "#     a = str(x[0]).strip()\n",
    "#     b = str(x[1]).strip()\n",
    "#     if b.__contains__(a):\n",
    "#         b = b.replace(a, '')\n",
    "#     return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['make'] = data[['model', 'make']].apply(make_rm_model, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = []\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in ['gender', 'age', 'province', 'city', 'model', 'make']:\n",
    "    data['{}_category'.format(col)] = le.fit_transform(data[col])\n",
    "    used_features.append('{}_category'.format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "emb_size = 16\n",
    "\n",
    "sentences = data['tagid'].values.tolist()\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [str(x) for x in sentences[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=emb_size, window=5, min_count=5, sg=0, hs=1, seed=42)\n",
    "emb_matrix = []\n",
    "for seq in sentences:\n",
    "    vec = []\n",
    "    for w in seq:\n",
    "        if w in model.wv:\n",
    "            vec.append(model.wv.get_vector(w))\n",
    "    if len(vec) > 0:\n",
    "        emb_matrix.append(np.mean(vec, axis=0))# 取求和平均的embedding？？为啥要这样\n",
    "    else:\n",
    "        emb_matrix.append([0] * emb_size)\n",
    "emb_matrix = np.array(emb_matrix)\n",
    "\n",
    "for i in range(emb_size):\n",
    "    data['{}_emb_{}'.format('tagid', i)] = emb_matrix[:, i]\n",
    "    used_features.append('{}_emb_{}'.format('tagid', i))\n",
    "del model, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(['tagid','time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[:train_data.shape[0]]\n",
    "test=data[train_data.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_res = np.zeros(shape=(train.shape[0]))\n",
    "test_res = np.zeros(shape=(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city',\n",
       "       'model', 'make', 'gender_category', 'age_category', 'province_category',\n",
       "       'city_category', 'model_category', 'make_category', 'tagid_emb_0',\n",
       "       'tagid_emb_1', 'tagid_emb_2', 'tagid_emb_3', 'tagid_emb_4',\n",
       "       'tagid_emb_5', 'tagid_emb_6', 'tagid_emb_7', 'tagid_emb_8',\n",
       "       'tagid_emb_9', 'tagid_emb_10', 'tagid_emb_11', 'tagid_emb_12',\n",
       "       'tagid_emb_13', 'tagid_emb_14', 'tagid_emb_15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 模型：lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'verbose': 0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "imp_Df = pd.DataFrame()\n",
    "imp_Df['feature'] = used_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.770295\n",
      "[100]\tvalid_0's auc: 0.782066\n",
      "[150]\tvalid_0's auc: 0.784925\n",
      "[200]\tvalid_0's auc: 0.786333\n",
      "[250]\tvalid_0's auc: 0.787371\n",
      "[300]\tvalid_0's auc: 0.788072\n",
      "[350]\tvalid_0's auc: 0.788799\n",
      "[400]\tvalid_0's auc: 0.789186\n",
      "[450]\tvalid_0's auc: 0.789686\n",
      "[500]\tvalid_0's auc: 0.789968\n",
      "[550]\tvalid_0's auc: 0.78997\n",
      "Early stopping, best iteration is:\n",
      "[501]\tvalid_0's auc: 0.79001\n",
      "1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.769544\n",
      "[100]\tvalid_0's auc: 0.7823\n",
      "[150]\tvalid_0's auc: 0.786334\n",
      "[200]\tvalid_0's auc: 0.787963\n",
      "[250]\tvalid_0's auc: 0.788763\n",
      "[300]\tvalid_0's auc: 0.789336\n",
      "[350]\tvalid_0's auc: 0.789818\n",
      "[400]\tvalid_0's auc: 0.790315\n",
      "[450]\tvalid_0's auc: 0.790574\n",
      "[500]\tvalid_0's auc: 0.790721\n",
      "[550]\tvalid_0's auc: 0.790876\n",
      "[600]\tvalid_0's auc: 0.791041\n",
      "[650]\tvalid_0's auc: 0.791019\n",
      "Early stopping, best iteration is:\n",
      "[604]\tvalid_0's auc: 0.791068\n",
      "2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.770072\n",
      "[100]\tvalid_0's auc: 0.781164\n",
      "[150]\tvalid_0's auc: 0.783718\n",
      "[200]\tvalid_0's auc: 0.785642\n",
      "[250]\tvalid_0's auc: 0.786646\n",
      "[300]\tvalid_0's auc: 0.786923\n",
      "[350]\tvalid_0's auc: 0.78753\n",
      "[400]\tvalid_0's auc: 0.787851\n",
      "[450]\tvalid_0's auc: 0.787818\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalid_0's auc: 0.78789\n",
      "3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.768838\n",
      "[100]\tvalid_0's auc: 0.780698\n",
      "[150]\tvalid_0's auc: 0.784095\n",
      "[200]\tvalid_0's auc: 0.78519\n",
      "[250]\tvalid_0's auc: 0.786475\n",
      "[300]\tvalid_0's auc: 0.787117\n",
      "[350]\tvalid_0's auc: 0.787864\n",
      "[400]\tvalid_0's auc: 0.787886\n",
      "Early stopping, best iteration is:\n",
      "[359]\tvalid_0's auc: 0.787983\n",
      "4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's auc: 0.770925\n",
      "[100]\tvalid_0's auc: 0.78332\n",
      "[150]\tvalid_0's auc: 0.786296\n",
      "[200]\tvalid_0's auc: 0.788125\n",
      "[250]\tvalid_0's auc: 0.789129\n",
      "[300]\tvalid_0's auc: 0.790139\n",
      "[350]\tvalid_0's auc: 0.790766\n",
      "[400]\tvalid_0's auc: 0.791017\n",
      "[450]\tvalid_0's auc: 0.791417\n",
      "[500]\tvalid_0's auc: 0.791753\n",
      "[550]\tvalid_0's auc: 0.79187\n",
      "Early stopping, best iteration is:\n",
      "[518]\tvalid_0's auc: 0.791913\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, valid_index) in enumerate(skf.split(train, train['label'])):\n",
    "    \n",
    "    X_train, X_valid = train.iloc[train_index][used_features].values, train.iloc[valid_index][used_features].values\n",
    "    y_train, y_valid = train.iloc[train_index]['label'], train.iloc[valid_index]['label']\n",
    "    #print(index)\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dval = lgb.Dataset(X_valid, label=y_valid)\n",
    "    lgb_model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[dval],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=50,\n",
    "    )\n",
    "    X_valid_pred = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
    "    imp_Df['cv'+str(index)] = lgb_model.feature_importance()\n",
    "\n",
    "    train_res[valid_index] = X_valid_pred\n",
    "    test_res = test_res + lgb_model.predict(test[used_features].values,\n",
    "                                            num_iteration=lgb_model.best_iteration) / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7115933333333333\n"
     ]
    }
   ],
   "source": [
    "train['predict'] = train_res\n",
    "train['rank'] = train['predict'].rank()\n",
    "train['p'] = 1\n",
    "train.loc[train['rank'] <= train.shape[0] * 0.5, 'p'] = 0\n",
    "best_f1_train = f1_score(train['label'].values, train['p'].values)\n",
    "print(best_f1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = test[['pid']]\n",
    "submit['rank'] = test_res\n",
    "submit.columns = ['user_id', 'rank']\n",
    "\n",
    "submit['rank'] = submit['rank'].rank()\n",
    "submit['category_id'] = 1\n",
    "submit.loc[submit['rank'] <= int(submit.shape[0] * 0.5), 'category_id'] = 0\n",
    "\n",
    "submit[['user_id', 'category_id']].to_csv('f1_{}.csv'.format(str(best_f1_train).split('.')[1]), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
