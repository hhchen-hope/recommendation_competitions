{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2dd147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5da89",
   "metadata": {},
   "source": [
    "# 1 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8205fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./final_train.txt', header=None)\n",
    "test = pd.read_csv('./final_test.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cbbfacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "test.columns = ['pid', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "train['label'] = train['label'].astype(int)\n",
    "data = pd.concat([train,test])\n",
    "data['label'] = data['label'].fillna(-1)\n",
    "data['tagid']=data['tagid'].fillna('[0]')\n",
    "data['time']=data['time'].fillna('[0]')\n",
    "data['age'] = data['age'].fillna(-1)\n",
    "data['gender'] = data['gender'].fillna(-1)\n",
    "\n",
    "data['age'] = data['age'].astype('str')\n",
    "data['gender'] = data['gender'].astype('str')\n",
    "data['tagid'] = data['tagid'].apply(lambda x:eval(x))\n",
    "data['time'] = data['time'].apply(lambda x: eval(x)) # str -> list\n",
    "data['tagid'] = data['tagid'].apply(lambda x:[str(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fa0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "MAX_NB_WORDS = 224254 \n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "# 训练word2vec，这里可以考虑elmo，bert等预训练\n",
    "w2v_model = Word2Vec(sentences=data['tagid'].tolist(), vector_size=embed_size, window=5, min_count=1,epochs=10)\n",
    "# 这里是划分训练集和测试数据\n",
    "X_train = data[:train.shape[0]]['tagid']\n",
    "X_test = data[train.shape[0]:]['tagid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b078fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建词典，利用了tf.keras的API，其实就是编码一下，具体可以看看API的使用方法\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a25592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 224254 word vectors.\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "# 计算一共出现了多少个单词，其实MAX_NB_WORDS我直接就用了这个数据\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "print('Total %s word vectors.' % nb_words)\n",
    "# 构建一个embedding的矩阵，之后输入到模型使用\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "y_categorical = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d50ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    embedding_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(nb_words,\n",
    "                         embed_size,\n",
    "                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                         weights=[embedding_matrix],\n",
    "                         trainable=False\n",
    "                         )\n",
    "    embed = embedder(embedding_input)\n",
    "    l = LSTM(128)(embed)\n",
    "    flat = BatchNormalization()(l)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(1, activation='sigmoid')(drop)\n",
    "    model = Model(inputs=embedding_input, outputs=main_output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 五折交叉验证 ---> 十折\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2019)\n",
    "oof = np.zeros([len(train), 1])\n",
    "predictions = np.zeros([len(test), 1])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "    print(\"fold n{}\".format(fold_ + 1))\n",
    "    model = my_model()\n",
    "    if fold_ == 0:\n",
    "        model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    bst_model_path = \"./{}.h10\".format(fold_)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    X_tra, X_val = X_train[trn_idx], X_train[val_idx]\n",
    "    y_tra, y_val = y_categorical[trn_idx], y_categorical[val_idx]\n",
    "\n",
    "    model.fit(X_tra, y_tra,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=128, batch_size=256, shuffle=True,\n",
    "              callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    model.load_weights(bst_model_path)\n",
    "\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "\n",
    "    predictions += model.predict(X_test) / folds.n_splits\n",
    "    print(predictions)\n",
    "    del model\n",
    "\n",
    "train['predict'] = oof\n",
    "train['rank'] = train['predict'].rank()\n",
    "train['p'] = 1\n",
    "train.loc[train['rank'] <= train.shape[0] * 0.5, 'p'] = 0\n",
    "bst_f1_tmp = f1_score(train['label'].values, train['p'].values)\n",
    "print(bst_f1_tmp)\n",
    "\n",
    "submit = test[['pid']]\n",
    "submit['tmp'] = predictions\n",
    "submit.columns = ['user_id', 'tmp']\n",
    "\n",
    "submit['rank'] = submit['tmp'].rank()\n",
    "submit['category_id'] = 1\n",
    "submit.loc[submit['rank'] <= int(submit.shape[0] * 0.5), 'category_id'] = 0\n",
    "\n",
    "print(submit['category_id'].mean())\n",
    "\n",
    "submit[['user_id', 'category_id']].to_csv('sorted_lstm_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
